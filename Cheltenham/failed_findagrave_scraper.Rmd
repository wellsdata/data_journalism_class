---
author: "Rob Wells/Sean Mussenden"
output:
  html_document: default
  pdf_document: default
---
# failed attempt at scraping the data from the findagrave due to the multiple pages being served. see scrape-pdf for the success

Scrape Cheltenham page for names

## Thanks to Sean Mussenden for his Advanced rvest tutorial, which this is shamelessly and ruthlessly stolen and remixed

https://github.com/smussenden/datajournalismbook


```{r}
# install.packages("tidyverse")
# install.packages("rvest")
# install.packages("janitor")

library(tidyverse)
library(rvest)
library(janitor)
library(readxl)
```

Find A Grave Page
https://www.findagrave.com/cemetery/2454348/memorial-search?firstname=&middlename=&lastname=&cemeteryName=Boy%27s+Village+of+Maryland+Cemetery&birthyear=&birthyearfilter=&deathyear=&deathyearfilter=&bio=&linkedToName=&plot=&memorialid=&mcid=&datefilter=&orderby=n&page=1#sr-250171171 

Control + left click at top of entries, Inspect
navigate to the developer console and find the selection that captures all of the data. left click, copy full xpath


```{r}
library(rvest)
library(httr)
library(xml2)
library(stringr)
library(dplyr)

# Try with longer delays and different approach
scrape_all_pages_alternative <- function() {
  base_url <- "https://www.findagrave.com/cemetery/2454348/memorial-search?firstname=&middlename=&lastname=&cemeteryName=Boy%27s+Village+of+Maryland+Cemetery&birthyear=&birthyearfilter=&deathyear=&deathyearfilter=&bio=&linkedToName=&plot=&memorialid=&mcid=&datefilter=&orderby=n&page="
  
  all_data <- data.frame(Box = character(), Folder = character(), Contents = character())
  
  for (page_num in 1:6) {
    cat("Attempting page", page_num, "with extended delay...\n")
    
    # Longer delay between pages
    if(page_num > 1) Sys.sleep(5)
    
    # Try multiple XPath patterns
    xpaths_to_try <- c(
      "/html/body/div[1]/section/div[2]/div/div/div[1]/div/div[3]/div",
      "//div[contains(@class, 'search-results')]//div",
      "//div[contains(@class, 'memorial')]",
      "//section//div//div//div"
    )
    
    url <- paste0(base_url, page_num, "#sr-250171171")
    
    success <- FALSE
    for(xpath in xpaths_to_try) {
      tryCatch({
        response <- GET(
          url,
          timeout(20),
          add_headers(
            `User-Agent` = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
            `Accept` = "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            `Accept-Language` = "en-US,en;q=0.9"
          )
        )
        
        if (status_code(response) == 200) {
          page_content <- read_html(rawToChar(response$content))
          target_content <- page_content %>%
            html_nodes(xpath = xpath) %>%
            html_text(trim = TRUE)
          
          if(length(target_content) > 0 && any(nchar(target_content) > 0)) {
            cat("  Success with XPath:", xpath, "- found", length(target_content), "elements\n")
            
            # Process this content (using your existing function)
            page_data <- process_page_content_simple(target_content)
            if(nrow(page_data) > 0) {
              all_data <- rbind(all_data, page_data)
              success <- TRUE
              break
            }
          }
        }
      }, error = function(e) {
        # Continue to next XPath
      })
    }
    
    if(!success) {
      cat("  No data found on page", page_num, "with any XPath\n")
      
      # Try the alternative URL format for this page
      alt_url <- paste0("https://www.findagrave.com/cemetery/2454348/memorial-search?firstname=&middlename=&lastname=&cemeteryName=Boy%27s+Village+of+Maryland+Cemetery&birthyear=&birthyearfilter=&deathyear=&deathyearfilter=&bio=&linkedToName=&plot=&memorialid=&mcid=&datefilter=&orderby=n&offset=", (page_num-1)*20)
      
      cat("  Trying offset-based URL...\n")
      tryCatch({
        response <- GET(alt_url, timeout(20))
        if (status_code(response) == 200) {
          page_content <- read_html(rawToChar(response$content))
          target_content <- page_content %>%
            html_nodes(xpath = xpaths_to_try[1]) %>%
            html_text(trim = TRUE)
          
          if(length(target_content) > 0) {
            page_data <- process_page_content_simple(target_content)
            if(nrow(page_data) > 0) {
              all_data <- rbind(all_data, page_data)
              cat("  Success with offset URL - found", nrow(page_data), "records\n")
            }
          }
        }
      }, error = function(e) {
        cat("  Offset URL also failed\n")
      })
    }
  }
  
  return(all_data)
}

# Simplified processing function
process_page_content_simple <- function(content) {
  if (is.null(content) || length(content) == 0) {
    return(data.frame(Box = character(), Folder = character(), Contents = character()))
  }
  
  # Combine all content and split by lines
  all_text <- paste(content, collapse = "\n")
  lines <- unlist(strsplit(all_text, "\n"))
  lines <- lines[trimws(lines) != ""]
  
  if(length(lines) == 0) {
    return(data.frame(Box = character(), Folder = character(), Contents = character()))
  }
  
  # Simple approach - just capture all non-empty lines
  return(data.frame(
    Box = NA,
    Folder = NA,
    Contents = lines,
    stringsAsFactors = FALSE
  ))
}

# Run the alternative scraper
cat("Starting alternative scraping approach...\n")
result_data <- scrape_all_pages_alternative()

if(nrow(result_data) > 0) {
  write.csv(result_data, "alternative_scrape_results.csv", row.names = FALSE)
  cat("Results saved. Total records:", nrow(result_data), "\n")
  print(head(result_data, 10))
} else {
  cat("No data collected with alternative approach either.\n")
  cat("This suggests the issue might be:\n")
  cat("1. JavaScript-rendered content (need selenium)\n")
  cat("2. Anti-scraping measures\n") 
  cat("3. Pages 2+ don't actually exist\n")
  cat("4. Different authentication/session requirements\n")
}
```

#Cleaning the df
```{r}
simplified_df <- result_data
simplified_df <- read_csv("alternative_scrape_results.csv")

# head(simplified_df, 20)
# A	Flowers have been left.
# &bullet; No grave photo

simplified_df <- simplified_df |> 
  filter(!str_detect(Contents, "Flowers have been left.")) |> 
  filter(!str_detect(Contents, "&bullet; No grave photo")) 

# delete rows 61-142

simplified_df <- slice(simplified_df, -c(61:142))

#delete 119-128

```


Claude.ai prompt
take the following .csv file, create a new row for name and date. do not include another text from the file. each name begins after the entry "no grave photo" . produce a .csv with the output. there should be 113 names in the final output.


```{r}
cleaned_df <- read_csv("extracted_names_dates.csv")



```



#old code, works but only one page at a time
```{r}
# Load required packages
library(rvest)
library(httr)
library(xml2)
library(stringr)
library(dplyr)

# Define the URL
url2 <- "https://www.findagrave.com/cemetery/2454348/memorial-search?firstname=&middlename=&lastname=&cemeteryName=Boy%27s+Village+of+Maryland+Cemetery&birthyear=&birthyearfilter=&deathyear=&deathyearfilter=&bio=&linkedToName=&plot=&memorialid=&mcid=&datefilter=&orderby=n&page=1#sr-250171171"


xpath_query <- "/html/body/div[1]/section/div[2]/div/div/div[1]/div/div[3]/div"



# Function to safely scrape webpage with error handling
safe_scrape <- function(url, xpath) {
  tryCatch({
    response <- GET(
      url,
      timeout(10),
      add_headers(
        `User-Agent` = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
        `Accept` = "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8"
      )
    )
    
    if (status_code(response) == 200) {
      page_content <- read_html(rawToChar(response$content))
      target_content <- page_content %>%
        html_nodes(xpath = xpath) %>%
        html_text(trim = TRUE)
      
      return(target_content)
    } else {
      stop(paste("HTTP error:", status_code(response)))
    }
  }, error = function(e) {
    message("Error occurred: ", e$message)
    return(NULL)
  })
}

# Function to extract box and folder numbers from content
extract_location <- function(content) {
  box_match <- str_extract(content, "box\\s+\\d+")
  folder_match <- str_extract(content, "folder\\s+\\d+")
  
  box_num <- if (!is.null(box_match)) {
    str_extract(box_match, "\\d+")
  } else {
    NA
  }
  
  folder_num <- if (!is.null(folder_match)) {
    str_extract(folder_match, "\\d+")
  } else {
    NA
  }
  
  return(list(box = box_num, folder = folder_num))
}

# Try to scrape the specific content
content <- safe_scrape(url2, xpath_query)

if (!is.null(content) && length(content) > 0) {
  # Split content into lines and remove empty lines
  lines <- unlist(strsplit(content, "\n"))
  lines <- lines[trimws(lines) != ""]
  
  # Initialize vectors
  boxes <- character()
  folders <- character()
  contents <- character()
  
  # Keep track of current box and folder
  current_box <- NA
  current_folder <- NA
  
  # Process each line
  for (line in lines) {
    # Check for box or folder information
    location_info <- extract_location(line)
    
    # Update current box/folder if found
    if (!is.na(location_info$box)) current_box <- location_info$box
    if (!is.na(location_info$folder)) current_folder <- location_info$folder
    
    # Skip lines that only contain box/folder information
    if (!grepl("^box\\s+\\d+$|^folder\\s+\\d+$", trimws(line))) {
      boxes <- c(boxes, current_box)
      folders <- c(folders, current_folder)
      contents <- c(contents, trimws(line))
    }
  }
  
  # Create data frame
  df <- data.frame(
    Box = boxes,
    Folder = folders,
    Contents = contents,
    stringsAsFactors = FALSE
  )
  
  # Write to CSV
  write.csv(df, "cheltenham_contents.csv", row.names = FALSE)
  cat("Content has been saved to 'cheltenham_contents.csv'\n")
  
  # Display first few rows
  print(head(df))
  
} else if (length(content) == 0) {
  cat("No content found at the specified XPath. The structure might have changed or the XPath might be incorrect.")
} else {
  cat("Failed to scrape the page. Please check the URL or try again later.")
}
```

Claude.ai query to restructure the data:


#Filtered and cleaned

```{r}
# Reshape the dataframe
result <- df %>%
  group_by(Box, Folder) %>%
  summarize(
    Box1 = first(Contents),
    Content1 = nth(Contents, 2),
    Year = nth(Contents, 3),.groups = 'drop'
  )

# View the result
# print(result)
# 
# write.csv(result, "full_moley_index.csv")
```

```{r}

months_pattern <- paste0("^(January|February|March|April|May|June|July|August|September|October|November|December)")


result <- result |> 
  janitor::clean_names() |> 
  separate(content1, c("last", "first"), sep = ",") |> 
  rename(location = box1) |> 
  mutate(year1 = ifelse(str_detect(year, "^19"), year, NA)) |> 
  mutate(date = ifelse(str_detect(year, months_pattern), year, NA)) |> 
  mutate(date = as.Date(parse_date_time(date, orders = c("mdy", "mdY", "Bdy")))) |> 
  mutate(year2 = year(date)) |> 
  mutate(year = coalesce(as.numeric(year1), as.numeric(year2))) |> 
  select(-year1, -year2)

  
```



# MERGE with existing notes

```{r}
#import full index: 6564 items
letters <- rio::import("/Users/robwells/Library/CloudStorage/Dropbox/Current_Projects/Moley project 2024/Hoover Searches/Hoover Moley Search Requests Aug 8 2024.xlsx") |> 
  janitor::clean_names() |> 
  mutate(box = as.character(box))

#130 rows with content
letters1 <- letters |> 
  filter(august_priority_1_5 !="NA" | notes != "NA" | select != "NA")

```


```{r}
letters |> 
  count(year) |> 
  arrange(desc(n))



```



# Speeches
```{r}
 speech <- letters |> 
  filter(str_detect(last,regex("address|speech", ignore_case = TRUE)))
```


```{r}

speech |> 
  count(year) |> 
  filter(year != "NA") |> 
   ggplot() +
  geom_col(aes(x=year, y=n, fill=n)) +
   theme(axis.text.x = element_text(angle=90))  +
  theme(legend.position = "none") +
    labs(title = "Raymond Moley Speeches",
       subtitle = "750 speeches 1914-1968",
       caption = "137 speeches lacked date entry. Graphic by Rob Wells, 12-13-2024",
       y="Count",
       x="Year")
  
 
```

```{r}
speech |> 
  count(year) |> 
  arrange(desc(n))
```



# NOTES BELOW 


```{r}
hoover <- df |> 
  filter(!str_detect(Contents, regex("box", ignore_case = TRUE))) |> 
# Identify rows where Contents contain a year
  mutate(
    Year = ifelse(grepl("^[0-9]{4}(-[0-9]{4})?$", Contents), Contents, NA)
  )

# Shift the Year values up by one row
hoover <- hoover %>%
  mutate(Year = lag(Year)) %>%
  filter(!is.na(Year))

# Remove rows where Contents are years
hoover <- hoover%>%
  filter(!grepl("^[0-9]{4}(-[0-9]{4})?$", Contents))


```
#this almost worked
```{r}
# Load required packages
library(rvest)
library(httr)
library(xml2)
library(stringr)
library(dplyr)

# Define the URL
url2 <- "https://oac.cdlib.org/findaid/ark:/13030/tf6779n7xj/entire_text/"
xpath_query <- "/html/body/div/div/div[2]/div[1]/div/div[31]"

# Function to safely scrape webpage with error handling
safe_scrape <- function(url, xpath) {
  tryCatch({
    # Make the request with proper headers and timeout
    response <- GET(
      url,
      timeout(10),
      add_headers(
        `User-Agent` = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
        `Accept` = "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8"
      )
    )
    
    # Check if request was successful
    if (status_code(response) == 200) {
      # Convert response to text and then parse HTML
      page_content <- read_html(rawToChar(response$content))
      
      # Extract content using XPath
      target_content <- page_content %>%
        html_nodes(xpath = xpath) %>%
        html_text(trim = TRUE)
      
      return(target_content)
    } else {
      stop(paste("HTTP error:", status_code(response)))
    }
  }, error = function(e) {
    message("Error occurred: ", e$message)
    return(NULL)
  })
}

# Function to parse location string into box and folder
parse_location <- function(text) {
  # Extract box number
  box_match <- str_match(text, "box\\s+(\\d+)")
  box_num <- if (!is.na(box_match[1])) box_match[2] else NA
  
  # Extract folder number
  folder_match <- str_match(text, "folder\\s+(\\d+)")
  folder_num <- if (!is.na(folder_match[1])) folder_match[2] else NA
  
  return(list(box = box_num, folder = folder_num))
}

# Try to scrape the specific content
content <- safe_scrape(url2, xpath_query)

if (!is.null(content) && length(content) > 0) {
  # Split content into lines
  lines <- unlist(strsplit(content, "\n"))
  
  # Initialize empty vectors
  boxes <- character()
  folders <- character()
  contents <- character()
  
  # Process each line
  for (line in lines) {
    if (trimws(line) != "") {  # Skip empty lines
      # Split line into location and content
      parts <- strsplit(line, "\\s{2,}")[[1]]
      
      if (length(parts) >= 2) {
        # Parse location
        location_info <- parse_location(parts[1])
        
        # Store data
        boxes <- c(boxes, location_info$box)
        folders <- c(folders, location_info$folder)
        contents <- c(contents, paste(parts[-1], collapse = " "))
      }
    }
  }
  
  # Create data frame
  df <- data.frame(
    Box = boxes,
    Folder = folders,
    Contents = contents,
    stringsAsFactors = FALSE
  )
  
  # Write to CSV
  write.csv(df, "archive_contents.csv", row.names = FALSE)
  cat("Content has been saved to 'archive_contents.csv'\n")
  
} else if (length(content) == 0) {
  cat("No content found at the specified XPath. The structure might have changed or the XPath might be incorrect.")
} else {
  cat("Failed to scrape the page. Please check the URL or try again later.")
}
```














